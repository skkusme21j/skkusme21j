{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQMacynBmD0cmmLzrxRJVt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skkusme21j/skkusme21j/blob/main/%EC%9E%90%EC%9C%A8%EC%A3%BC%ED%96%89.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XMX_Mfqm9hfL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm # Displays a progress bar\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import Dataset, Subset, DataLoader, random_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nw5WCKC9mIv",
        "outputId": "e16711a7-b0b5-4f2e-9c8c-6276a4cdbf0d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "def read_images(folder_path):\n",
        "    images = []\n",
        "    image_files = sorted(os.listdir(folder_path))\n",
        "    for file_name in image_files:\n",
        "        image_path = os.path.join(folder_path, file_name)\n",
        "        image = Image.open(image_path)  # 이미지 읽어오기\n",
        "        images.append(image)\n",
        "    return images\n",
        "\n",
        "def read_labels(label_file):\n",
        "    df = pd.read_csv(label_file)\n",
        "    image_filenames = df['center'].tolist()\n",
        "    steering_angles = df['steering'].tolist()\n",
        "    labels = [float(angle) for angle in steering_angles]\n",
        "    return labels\n",
        "\n",
        "# def read_labels(label_file):\n",
        "#     df = pd.read_csv(label_file)\n",
        "#     image_filenames = df['center'].tolist()\n",
        "#     steering_angles = df['steering'].tolist()\n",
        "#     labels = [name[6:] for name in image_filenames]\n",
        "#     return labels\n",
        "  \n",
        "def image_titles(folder_path):\n",
        "  titles=[]\n",
        "  image_files=sorted(os.listdir(folder_path))\n",
        "  for file_name in image_files:\n",
        "    titles.append(file_name)\n",
        "  return titles\n",
        "\n",
        "\n",
        "def augment_data(images,labels):\n",
        "  augmented_images = []\n",
        "  augmented_labels = []\n",
        "\n",
        "  for image, label in zip(images,labels):\n",
        "    augmented_images.append(image)  # 원본 이미지 추가\n",
        "    # Create a horizontal flip transform\n",
        "    transform = transforms.RandomHorizontalFlip(p=1.0)  # p=1.0 to always perform the flip\n",
        "    # Apply the horizontal flip to the image\n",
        "    flipped_image = transform(image)\n",
        "    augmented_images.append(flipped_image)\n",
        "    augmented_labels.append(label) \n",
        "    # 가로 반전 (horizontal flipping)\n",
        "    flipped_label = -label   # steering wheel angle 부호 반전\n",
        "    augmented_labels.append(flipped_label)\n",
        "\n",
        "  return augmented_images, augmented_labels\n",
        "\n",
        "def crop_image(image_path):\n",
        "    # Load the image\n",
        "    img = Image.open(image_path)\n",
        "\n",
        "    # Calculate crop sizes\n",
        "    width, height = img.size\n",
        "    crop_top = int(height * 0.35)\n",
        "    crop_bottom = int(height * 0.18)\n",
        "\n",
        "    # Define the transformation\n",
        "    transform = transforms.Compose([\n",
        "        transforms.CenterCrop((height - crop_top - crop_bottom, width)),\n",
        "    ])\n",
        "\n",
        "    # Apply the transformation\n",
        "    cropped_img = transform(img)\n",
        "\n",
        "    return cropped_img\n",
        "\n",
        "def crop_images_in_folder(image_folder):\n",
        "    # Create a list to store the cropped images\n",
        "    cropped_images = []\n",
        "    image_files = sorted(os.listdir(image_folder))\n",
        "\n",
        "    # Iterate over the files in the image folder\n",
        "    for filename in os.listdir(image_folder):\n",
        "        # Construct the image path\n",
        "        image_path = os.path.join(image_folder, filename)\n",
        "        image=crop_image(image_path)\n",
        "        a=np.array(image)\n",
        "        # Load the image\n",
        "        \n",
        "\n",
        "        # Append the cropped image to the list\n",
        "        cropped_images.append(a)\n",
        "\n",
        "    return cropped_images\n",
        "\n",
        "# A2D2 데이터셋 경로\n",
        "# dataset_path = '/content/drive/MyDrive/자율주행인공지능 및 제어/IMG'\n",
        "\n",
        "# A2D2 데이터셋의 이미지 폴더와 라벨 파일 경로 설정\n",
        "image_folder ='/content/drive/MyDrive/자율주행인공지능 및 제어/IMG'\n",
        "label_file = '/content/drive/MyDrive/자율주행인공지능 및 제어/자율주행 프로젝트/driving_log.csv'"
      ],
      "metadata": {
        "id": "_daDJ4YB9mhF"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# folder 생성\n",
        "def createFolder(directory):\n",
        "    try:\n",
        "        if not os.path.exists(directory):\n",
        "            os.makedirs(directory)\n",
        "    except OSError:\n",
        "        print ('Error: Creating directory. ' +  directory)"
      ],
      "metadata": {
        "id": "qEkVrCaT9xXz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "def save_image_pairs(dataset, save_folder):\n",
        "    # Create the save folder if it doesn't exist\n",
        "    os.makedirs(save_folder, exist_ok=True)\n",
        "\n",
        "    # Iterate over the dataset and save image-label pairs\n",
        "    for idx in range(len(dataset)):\n",
        "        # Get the image from the dataset\n",
        "        image = dataset[idx]\n",
        "\n",
        "        # Construct the file paths for saving\n",
        "        image_path = os.path.join(save_folder, f'image_{idx}.jpg')\n",
        "\n",
        "        # Save the image\n",
        "        image.save(image_path)\n",
        "\n",
        "        \n",
        "def save_label_pairs(dataset, save_folder):\n",
        "    # Create the save folder if it doesn't exist\n",
        "    os.makedirs(save_folder, exist_ok=True)\n",
        "\n",
        "    # Iterate over the dataset and save image-label pairs\n",
        "    for idx in range(len(dataset)):\n",
        "        # Get the label from the dataset\n",
        "        label = dataset[idx]\n",
        "\n",
        "        # Construct the file paths for saving\n",
        "        label_path = os.path.join(save_folder, f'label_{idx}.txt')\n",
        "\n",
        "\n",
        "        # Save the label (assuming it's a string or a single value)\n",
        "        with open(label_path, 'w') as file:\n",
        "            file.write(str(label))"
      ],
      "metadata": {
        "id": "B9hXZ69W-NZh"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "createFolder('./Dataset/Train')\n",
        "createFolder('./Dataset/Test')\n",
        "createFolder('./Dataset/Train/image')\n",
        "createFolder('./Dataset/Train/label')\n",
        "createFolder('./Dataset/Test/image')\n",
        "createFolder('./Dataset/Test/label')\n",
        "createFolder('./Dataset/Train/crop_image')\n",
        "createFolder('./Dataset/Test/crop_image')"
      ],
      "metadata": {
        "id": "G6C-rQyaACkC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class A2D2Dataset(Dataset):\n",
        "  def __init__(self,images,labels):\n",
        "    self.images=images\n",
        "    self.labels=labels\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    image=self.images[idx]\n",
        "    label=self.labels[idx]\n",
        "    return image,label"
      ],
      "metadata": {
        "id": "eGn7YRrwAyjt"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images=read_images(image_folder)\n",
        "labels=read_labels(label_file)\n",
        "aug_images, aug_labels=augment_data(images, labels)\n",
        "dataset=A2D2Dataset(aug_images,aug_labels)"
      ],
      "metadata": {
        "id": "Ck0nznYuBBi6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train, test=random_split(dataset,[0.8,0.2])\n",
        "train_indices, test_indices = random_split(range(len(dataset)), [0.8, 0.2])\n",
        "\n",
        "train_images = [dataset[i][0] for i in train_indices]\n",
        "train_labels = [dataset[i][1] for i in train_indices]\n",
        "\n",
        "test_images = [dataset[i][0] for i in test_indices]\n",
        "test_labels = [dataset[i][1] for i in test_indices]"
      ],
      "metadata": {
        "id": "z5G5mXjNDv6a"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 폴더에 저장\n",
        "save_image_pairs(train_images,'./Dataset/Train/image')\n",
        "save_image_pairs(test_images,'./Dataset/Test/image')\n",
        "save_label_pairs(train_labels,'./Dataset/Train/label')\n",
        "save_label_pairs(test_labels,'./Dataset/Test/label')"
      ],
      "metadata": {
        "id": "Byl-QmM7EOmB"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading datasets...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNJ5v-XwPaWY",
        "outputId": "281870f3-bce2-4e35-e274-2ab5c6aa65ff"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# crop image folder에 저\n",
        "def crop_image_in_folder(image_folder, output_folder):\n",
        "    # Create the output folder if it doesn't exist\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Iterate over the files in the image folder\n",
        "    for filename in os.listdir(image_folder):\n",
        "        # Construct the image path\n",
        "        image_path = os.path.join(image_folder, filename)\n",
        "\n",
        "        # Load the image\n",
        "        img = Image.open(image_path)\n",
        "\n",
        "        # Calculate crop sizes\n",
        "        width, height = img.size\n",
        "        crop_top = int(height * 0.35)\n",
        "        crop_bottom = int(height * 0.18)\n",
        "\n",
        "        # Define the transformation\n",
        "        transform = transforms.Compose([\n",
        "            transforms.CenterCrop((height - crop_top - crop_bottom, width)),\n",
        "        ])\n",
        "\n",
        "        # Apply the transformation\n",
        "        cropped_img = transform(img)\n",
        "\n",
        "        # Construct the output file path\n",
        "        output_path = os.path.join(output_folder, filename)\n",
        "\n",
        "        # Save the cropped image\n",
        "        cropped_img.save(output_path)\n",
        "\n",
        "crop_image_in_folder('./Dataset/Train/image','./Dataset/Train/image')\n",
        "crop_image_in_folder('./Dataset/Test/image','./Dataset/Test/image')"
      ],
      "metadata": {
        "id": "alTrUt2wQzBU"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MyTransform = transforms.Compose([\n",
        "    transforms.ToTensor(), # Transform from [0,255] uint8 to [0,1] float\n",
        "    transforms.Normalize([0.5], [0.5]) # TODO: Normalize to zero mean and unit variance with appropriate parameters\n",
        "])"
      ],
      "metadata": {
        "id": "hoyzRS83TyXH"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_all_label_files(folder_path):\n",
        "    text_contents = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        if os.path.isfile(file_path) and filename.endswith('.txt'):\n",
        "            with open(file_path, 'r') as file:\n",
        "                content = file.read()\n",
        "            text_contents.append(content)\n",
        "    return text_contents"
      ],
      "metadata": {
        "id": "WEFGzzS1Wl4E"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class A2D2(Dataset):\n",
        "  def __init__(self,image_folder,label_folder):\n",
        "    self.images=read_images(image_folder)\n",
        "    self.labels=read_all_label_files(label_folder)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    image=self.images[idx]\n",
        "    label=self.labels[idx]\n",
        "    return image,label\n",
        " \n",
        "real_train_dataset = A2D2('./Dataset/Train/image','./Dataset/Train/label')\n",
        "real_test_dataset = A2D2('./Dataset/Test/image','./Dataset/Test/label')"
      ],
      "metadata": {
        "id": "eMZ0ufz0UsCV"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "createFolder('./DATASET')\n",
        "createFolder('./DATASET/Train')\n",
        "createFolder('./DATASET/Test')"
      ],
      "metadata": {
        "id": "P9vk-OwxpZIw"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def delete_folder(folder_path):\n",
        "    # 폴더 안에 있는 모든 파일 삭제\n",
        "    for filename in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        if os.path.isfile(file_path):\n",
        "            os.remove(file_path)\n",
        "        elif os.path.isdir(file_path):\n",
        "            shutil.rmtree(file_path)\n",
        "\n",
        "    # 폴더 삭제\n",
        "    shutil.rmtree(folder_path)\n",
        "\n",
        "folder_path = '/content/DATASET/Test'\n",
        "delete_folder(folder_path)"
      ],
      "metadata": {
        "id": "nitmFij3tWBc"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_folder = '/content/DATASET/Train'  # 저장할 폴더 경로 설정\n",
        "\n",
        "for idx, (image, label) in enumerate(real_train_dataset):\n",
        "    # Set the filename using the label\n",
        "    filename = \"label/\" +label + \".jpg\"\n",
        "\n",
        "    # Construct the full file paths\n",
        "    image_path = os.path.join(save_folder, filename)\n",
        "    os.makedirs(os.path.dirname(image_path), exist_ok=True)\n",
        "    # Save the image\n",
        "    image.save(image_path)"
      ],
      "metadata": {
        "id": "bydIAlPTpzwh"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_folder = '/content/DATASET/Test'  # 저장할 폴더 경로 설정\n",
        "\n",
        "for idx, (image, label) in enumerate(real_test_dataset):\n",
        "    # Set the filename using the label\n",
        "    filename = \"label/\" +label + \".jpg\"\n",
        "\n",
        "    # Construct the full file paths\n",
        "    image_path = os.path.join(save_folder, filename)\n",
        "    os.makedirs(os.path.dirname(image_path), exist_ok=True)\n",
        "    # Save the image\n",
        "    image.save(image_path)\n"
      ],
      "metadata": {
        "id": "zUG2w4y1scwQ"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_train = datasets.ImageFolder(root='./DATASET/Train', transform=MyTransform, target_transform=False)\n",
        "Data_test = datasets.ImageFolder(root='./DATASET/Test', transform=MyTransform, target_transform=False)"
      ],
      "metadata": {
        "id": "f5vIvk6cUzII"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainloader = DataLoader(Data_train, batch_size=128, shuffle=True)\n",
        "testloader = DataLoader(Data_test, batch_size=1, shuffle=True)"
      ],
      "metadata": {
        "id": "d-mAdsXrvKeT"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # TODO: [Transfer learning with pre-trained ResNet-50] Design your own fully-connected network (FCN).\n",
        "        # Design your own FCN for regression. Here I provide a sample of two-layer FCN.\n",
        "        # Refer to PyTorch documentations of torch.nn to pick your layers. (https://pytorch.org/docs/stable/nn.html)\n",
        "        # Some common Choices are: Linear, ReLU, Dropout, MaxPool2d, AvgPool2d\n",
        "        # If you have many layers, consider using nn.Sequential() to simplify your code\n",
        "        \n",
        "        # Load pretrained ResNet-50\n",
        "        self.model_resnet = models.resnet50(pretrained=True)\n",
        "                \n",
        "        # Set ResNet-50's FCN as an identity mapping\n",
        "        num_fc_in = self.model_resnet.fc.in_features\n",
        "        self.model_resnet.fc = nn.Identity()\n",
        "        \n",
        "        # TODO: Design your own FCN\n",
        "        self.fc1 = nn.Linear(num_fc_in, 256, bias = True) # from input of size num_fc_in to output of size ?\n",
        "        self.fc2 = nn.Linear(256, 1, bias = True) # from hidden layer to steering angle\n",
        "\n",
        "        # CNN\n",
        "        self.conv1 = nn.Conv2d(3, 3, kernel_size=5, padding=2)\n",
        "        self.conv2 = nn.Conv2d(3, 5, kernel_size=5, padding=2)\n",
        "\n",
        "        # YOLO\n",
        "        \n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        # TODO: Design your own network, implement forward pass here\n",
        "        \n",
        "        relu = nn.ReLU() # No need to define self.relu because it contains no parameters\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            features = self.model_resnet(x)\n",
        "            \n",
        "        x = self.fc1(features) # Activation are flattened before being passed to the fully connected layers\n",
        "        x = relu(x)\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        x_cnn = self.conv1(x)\n",
        "        x_cnn = nn.ReLU()(x_cnn)\n",
        "        x_cnn = self.conv2(x_cnn)\n",
        "        x_cnn = nn.ReLU()(x_cnn)\n",
        "        x_cnn = torch.flatten(x_cnn, 1)\n",
        "        \n",
        "        # The loss layer will be applied outside Network class\n",
        "        return x_cnn"
      ],
      "metadata": {
        "id": "Mx5rLAzevMao"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}